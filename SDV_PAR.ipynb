{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4c28fb7-f942-45c3-a76a-9019e3920079",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\raghu\\AppData\\Local\\Temp\\ipykernel_12008\\2549332448.py:34: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  df_hourly = df.resample('H').mean().reset_index()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"sequence_key\": \"sequence_id\",\n",
      "    \"columns\": {\n",
      "        \"timestamp\": {\n",
      "            \"sdtype\": \"datetime\"\n",
      "        },\n",
      "        \"sensor_id\": {\n",
      "            \"sdtype\": \"categorical\"\n",
      "        },\n",
      "        \"reading\": {\n",
      "            \"sdtype\": \"numerical\"\n",
      "        },\n",
      "        \"sequence_id\": {\n",
      "            \"sdtype\": \"id\"\n",
      "        }\n",
      "    },\n",
      "    \"METADATA_SPEC_VERSION\": \"SINGLE_TABLE_V1\"\n",
      "}\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 4.0.0 (0)\n",
       " -->\n",
       "<!-- Title: Metadata Pages: 1 -->\n",
       "<svg width=\"174pt\" height=\"100pt\"\n",
       " viewBox=\"0.00 0.00 174.00 100.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 96)\">\n",
       "<title>Metadata</title>\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-96 170,-96 170,4 -4,4\"/>\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title></title>\n",
       "<path fill=\"#ffec8b\" stroke=\"black\" d=\"M12,-0.5C12,-0.5 154,-0.5 154,-0.5 160,-0.5 166,-6.5 166,-12.5 166,-12.5 166,-79.5 166,-79.5 166,-85.5 160,-91.5 154,-91.5 154,-91.5 12,-91.5 12,-91.5 6,-91.5 0,-85.5 0,-79.5 0,-79.5 0,-12.5 0,-12.5 0,-6.5 6,-0.5 12,-0.5\"/>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-76.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">timestamp : datetime</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-61.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">sensor_id : categorical</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-46.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">reading : numerical</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-31.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">sequence_id : id</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"0,-23.5 166,-23.5 \"/>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-8.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Sequence key: sequence_id</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x245086608e0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sdv.metadata import SingleTableMetadata\n",
    "from sdv.sequential import PARSynthesizer\n",
    "import torch\n",
    "import gc\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import pickle\n",
    "\n",
    "# Clear GPU cache\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Garbage collection to free up memory\n",
    "gc.collect()\n",
    "\n",
    "# Verify if CUDA is available and set the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "# Load your data from a CSV file (replace this with the actual path to your CSV file)\n",
    "df = pd.read_csv('C:/Users/raghu/CS6140/PEMS-BAY.csv')\n",
    "\n",
    "# Convert the datetime column and handle parsing errors\n",
    "df['Unnamed: 0'] = pd.to_datetime(df['Unnamed: 0'], errors='coerce')\n",
    "\n",
    "# Remove rows with invalid datetime values\n",
    "df = df.dropna(subset=['Unnamed: 0'])\n",
    "\n",
    "# Rename the datetime column\n",
    "df.rename(columns={'Unnamed: 0': 'timestamp'}, inplace=True)\n",
    "\n",
    "# Set the timestamp column as the index\n",
    "df.set_index('timestamp', inplace=True)\n",
    "\n",
    "# Aggregate data to hourly intervals by computing the mean for each hour\n",
    "df_hourly = df.resample('H').mean().reset_index()\n",
    "\n",
    "# Melt the dataframe to have a long format\n",
    "data_long = df_hourly.melt(id_vars=['timestamp'], var_name='sensor_id', value_name='reading')\n",
    "\n",
    "# Sort the DataFrame by 'timestamp'\n",
    "data_long = data_long.sort_values(by='timestamp')\n",
    "\n",
    "# Create a combined sequence ID based on the timestamp and sensor_id\n",
    "data_long['sequence_id'] = data_long['timestamp'].astype(str) + '_' + data_long['sensor_id'].astype(str)\n",
    "\n",
    "# Define metadata\n",
    "metadata = SingleTableMetadata()\n",
    "\n",
    "# Add the columns to the metadata\n",
    "metadata.add_column('timestamp', sdtype='datetime')\n",
    "metadata.add_column('sensor_id', sdtype='categorical')\n",
    "metadata.add_column('reading', sdtype='numerical')\n",
    "metadata.add_column('sequence_id', sdtype='id')\n",
    "\n",
    "# Set the sequence key in the metadata\n",
    "metadata.set_sequence_key('sequence_id')\n",
    "print(metadata)\n",
    "# Visualize the metadata\n",
    "metadata.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adaf0274-b9a9-47f1-9991-2047c1b52c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the PARSynthesizer with CUDA\n",
    "synthesizer = PARSynthesizer(\n",
    "    metadata=metadata,\n",
    "    verbose=True,\n",
    "    cuda=True  # Use GPU\n",
    ")\n",
    "\n",
    "# Define sequence length\n",
    "sequence_length = 24  # 24 hourly intervals in a day\n",
    "\n",
    "# Fit the synthesizer to the data in segments to manage memory usage\n",
    "num_segments = 20  # Increase the number of segments to further reduce memory usage per segment\n",
    "segment_length = len(data_long) // num_segments\n",
    "\n",
    "def process_segment(i):\n",
    "    start_idx = i * segment_length\n",
    "    end_idx = start_idx + segment_length if i < num_segments - 1 else len(data_long)\n",
    "    data_segment = data_long.iloc[start_idx:end_idx]\n",
    "    \n",
    "    print(f\"Fitting segment {i+1}/{num_segments}, rows {start_idx} to {end_idx}\")\n",
    "    \n",
    "    synthesizer.fit(data_segment)\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    # Save the synthetic data segment to a CSV file\n",
    "    synthetic_data_segment = synthesizer.sample(num_sequences=len(data_segment['sequence_id'].unique()), sequence_length=sequence_length)\n",
    "    synthetic_data_segment.to_csv(f'synthetic_data_segment_{i+1}.csv', index=False)\n",
    "\n",
    "# Fit the synthesizer using threading\n",
    "with ThreadPoolExecutor(max_workers=3) as executor:  # Adjust max_workers as needed\n",
    "    futures = [executor.submit(process_segment, i) for i in range(num_segments)]\n",
    "    for future in as_completed(futures):\n",
    "        future.result()  # Wait for all futures to complete\n",
    "\n",
    "# Load and merge all synthetic data segments into a single DataFrame\n",
    "synthetic_data_all_segments = pd.concat(\n",
    "    [pd.read_csv(f'synthetic_data_segment_{i+1}.csv') for i in range(num_segments)],\n",
    "    ignore_index=True\n",
    ")\n",
    "\n",
    "# Save the merged synthetic data to a single CSV file\n",
    "synthetic_data_all_segments.to_csv('synthetic_data_merged.csv', index=False)\n",
    "\n",
    "# Print the merged synthetic data\n",
    "print(synthetic_data_all_segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "029e119a-fddd-4b6a-a8a3-11eb2332bb32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "0           2017-01-01\n",
      "1           2017-01-01\n",
      "2           2017-01-01\n",
      "3           2017-01-01\n",
      "4           2017-01-01\n",
      "               ...    \n",
      "16937695    2017-06-30\n",
      "16937696    2017-06-30\n",
      "16937697    2017-06-30\n",
      "16937698    2017-06-30\n",
      "16937699    2017-06-30\n",
      "Name: sequence_id, Length: 16937700, dtype: object\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 4.0.0 (0)\n",
       " -->\n",
       "<!-- Title: Metadata Pages: 1 -->\n",
       "<svg width=\"174pt\" height=\"100pt\"\n",
       " viewBox=\"0.00 0.00 174.00 100.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 96)\">\n",
       "<title>Metadata</title>\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-96 170,-96 170,4 -4,4\"/>\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title></title>\n",
       "<path fill=\"#ffec8b\" stroke=\"black\" d=\"M12,-0.5C12,-0.5 154,-0.5 154,-0.5 160,-0.5 166,-6.5 166,-12.5 166,-12.5 166,-79.5 166,-79.5 166,-85.5 160,-91.5 154,-91.5 154,-91.5 12,-91.5 12,-91.5 6,-91.5 0,-85.5 0,-79.5 0,-79.5 0,-12.5 0,-12.5 0,-6.5 6,-0.5 12,-0.5\"/>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-76.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">timestamp : datetime</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-61.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">sensor_id : categorical</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-46.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">reading : numerical</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-31.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">sequence_id : id</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"0,-23.5 166,-23.5 \"/>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-8.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Sequence key: sequence_id</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x22f5ebcf4c0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sdv.metadata import SingleTableMetadata\n",
    "from sdv.sequential import PARSynthesizer\n",
    "import torch\n",
    "import gc\n",
    "import pickle\n",
    "import os\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# Clear GPU cache\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Garbage collection to free up memory\n",
    "gc.collect()\n",
    "\n",
    "# Verify if CUDA is available and set the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load your data from a CSV file (replace this with the actual path to your CSV file)\n",
    "df = pd.read_csv('C:/Users/raghu/CS6140/PEMS-BAY.csv')\n",
    "\n",
    "# Convert the datetime column and handle parsing errors\n",
    "def parse_dates(x):\n",
    "    try:\n",
    "        return pd.to_datetime(x)\n",
    "    except:\n",
    "        return pd.NaT\n",
    "\n",
    "df['Unnamed: 0'] = df['Unnamed: 0'].apply(parse_dates)\n",
    "\n",
    "# Remove rows with invalid datetime values\n",
    "df = df.dropna(subset=['Unnamed: 0'])\n",
    "\n",
    "# Rename the datetime column\n",
    "df.rename(columns={'Unnamed: 0': 'timestamp'}, inplace=True)\n",
    "\n",
    "# Melt the dataframe to have a long format\n",
    "data_long = df.melt(id_vars=['timestamp'], var_name='sensor_id', value_name='reading')\n",
    "\n",
    "data_long['sequence_id'] = data_long['timestamp'].dt.date\n",
    "\n",
    "print(data_long['sequence_id'])\n",
    "# Define metadata\n",
    "metadata = SingleTableMetadata()\n",
    "\n",
    "# Add the columns to the metadata\n",
    "metadata.add_column('timestamp', sdtype='datetime')\n",
    "metadata.add_column('sensor_id', sdtype='categorical')\n",
    "metadata.add_column('reading', sdtype='numerical')\n",
    "metadata.add_column('sequence_id', sdtype='id')\n",
    "\n",
    "# Set the sequence key in the metadata\n",
    "metadata.set_sequence_key('sequence_id')\n",
    "\n",
    "# Visualize the metadata\n",
    "metadata.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8655f2-85f9-4f17-ba0a-ba979bceb818",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define sequence length\n",
    "sequence_length = 288  # Adjust this value as needed\n",
    "\n",
    "# Fit the synthesizer to the data in segments to manage memory usage\n",
    "num_segments = 20  # Increase the number of segments to further reduce memory usage per segment\n",
    "segment_length = len(data_long) // num_segments\n",
    "\n",
    "def process_segment(i):\n",
    "    # Initialize a new synthesizer for each thread\n",
    "    synthesizer = PARSynthesizer(\n",
    "        metadata=metadata,\n",
    "        verbose=True,\n",
    "        cuda=True  # Use GPU\n",
    "    )\n",
    "    \n",
    "    start_idx = i * segment_length\n",
    "    end_idx = start_idx + segment_length if i < num_segments - 1 else len(data_long)\n",
    "    data_segment = data_long.iloc[start_idx:end_idx]\n",
    "    \n",
    "    print(f\"Fitting segment {i+1}/{num_segments}, rows {start_idx} to {end_idx}\")\n",
    "    \n",
    "    synthesizer.fit(data_segment)\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    # Generate and save synthetic data after processing each segment\n",
    "    synthetic_data = synthesizer.sample(num_sequences=len(data_long['sequence_id'].unique()), sequence_length=sequence_length)\n",
    "    synthetic_data.to_csv(f'synthetic_data_segment_{i+1}.csv', index=False)\n",
    "    \n",
    "    # Print progress for every 5% of segments completed\n",
    "    if (i + 1) % (num_segments // 20) == 0:\n",
    "        print(f\"Progress: {((i + 1) / num_segments) * 100:.2f}%\")\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=2) as executor:  # Adjust max_workers as needed\n",
    "    futures = [executor.submit(process_segment, i) for i in range(num_segments)]\n",
    "    for future in as_completed(futures):\n",
    "        future.result()  # Wait for all futures to complete\n",
    "\n",
    "# Generate final synthetic data after all segments are processed\n",
    "synthesizer = PARSynthesizer(\n",
    "    metadata=metadata,\n",
    "    verbose=True,\n",
    "    cuda=True  # Use GPU\n",
    ")\n",
    "\n",
    "# Fit the synthesizer to the data again for consolidation (if needed)\n",
    "synthesizer.fit(data_long)\n",
    "\n",
    "synthetic_data = synthesizer.sample(num_sequences=len(data_long['sequence_id'].unique()), sequence_length=sequence_length)\n",
    "\n",
    "# Output the final synthetic data\n",
    "print(synthetic_data)\n",
    "\n",
    "# Save the final synthetic data to a CSV file\n",
    "synthetic_data.to_csv('synthetic_data_final.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950bf701-b431-44e9-8360-11c8880a8b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic_data_all_segments = pd.concat(\n",
    "    [pd.read_csv(f'synthetic_data_segment_{i+1}.csv') for i in range(num_segments)],\n",
    "    ignore_index=True\n",
    ")\n",
    "\n",
    "# Save the merged synthetic data to a single CSV file\n",
    "synthetic_data_all_segments.to_csv('synthetic_data_merged.csv', index=False)\n",
    "\n",
    "# Print the merged synthetic data\n",
    "print(synthetic_data_all_segments)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
